{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physical  Data Engineering, Part 2\n",
    "##### Notebook to engineer the second of two provided datasets of physical data by [Watford F.C](https://www.watfordfc.com/), using [Python](https://www.python.org/) [pandas](http://pandas.pydata.org/).\n",
    "\n",
    "### By [Edd Webster](https://www.twitter.com/eddwebster)\n",
    "Notebook first written: 11/02/2022<br>\n",
    "Notebook last updated: 12/02/2022\n",
    "\n",
    "![Watford F.C.](../../img/club_badges/premier_league/watford_fc_logo_small.png)\n",
    "\n",
    "Click [here](#section4) to jump straight into the Data Engineering section and skip the [Notebook Brief](#section2) and [Data Sources](#section3) sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "\n",
    "## <a id='introduction'>Introduction</a>\n",
    "This notebook engineers a second of two provided datasets of physical data by [Watford F.C](https://www.watfordfc.com/), using [pandas](http://pandas.pydata.org/) for data manipulation through DataFrames.\n",
    "\n",
    "For more information about this notebook and the author, I am available through all the following channels:\n",
    "*    [eddwebster.com](https://www.eddwebster.com/);\n",
    "*    edd.j.webster@gmail.com;\n",
    "*    [@eddwebster](https://www.twitter.com/eddwebster);\n",
    "*    [linkedin.com/in/eddwebster](https://www.linkedin.com/in/eddwebster/);\n",
    "*    [github/eddwebster](https://github.com/eddwebster/); and\n",
    "*    [public.tableau.com/profile/edd.webster](https://public.tableau.com/profile/edd.webster).\n",
    "\n",
    "A static version of this notebook can be found [here](https://nbviewer.org/github/eddwebster/watford/blob/main/notebooks/2_data_engineering/Physical%20Data%20Engineering%20Part%202.ipynb). This notebook has an accompanying [`watford`](https://github.com/eddwebster/watford) GitHub repository and for my full repository of football analysis, see my [`football_analysis`](https://github.com/eddwebster/football_analytics) GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## <a id='notebook_contents'>Notebook Contents</a>\n",
    "1.    [Notebook Dependencies](#section1)<br>\n",
    "2.    [Notebook Brief](#section2)<br>\n",
    "3.    [Data Sources](#section3)<br>\n",
    "      1.    [Introduction](#section3.1)<br>\n",
    "      2.    [Read in the Datasets](#section3.2)<br>\n",
    "      3.    [Initial Data Handling](#section3.3)<br>\n",
    "4.    [Data Engineering](#section4)<br>\n",
    "      1.    [Prepare Training Data](#section4.1)<br>\n",
    "      2.    [Split Out Unified Training Data into Individual Training Drills](#section4.2)<br>\n",
    "      3.    [Engineer DataFrame to Match Tracking Data Format](#section4.3)<br>\n",
    "      4.    [Calculate Speed, Distance, and Acceleration](#section4.4)<br>\n",
    "      5.    [Create Physical Reports for Each Individual Training Session](#section4.5)<br>\n",
    "      6.    [Create Single Physical Report for the Day of Interest](#section4.6)<br>\n",
    "5.    [Summary](#section5)<br>\n",
    "6.    [Next Steps](#section6)<br>\n",
    "7.    [References](#section7)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='section1'></a>\n",
    "\n",
    "## <a id='#section1'>1. Notebook Dependencies</a>\n",
    "\n",
    "This notebook was written using [Python 3](https://www.python.org/) and requires the following libraries:\n",
    "*    [`Jupyter notebooks`](https://jupyter.org/) for this notebook environment with which this project is presented;\n",
    "*    [`NumPy`](http://www.numpy.org/) for multidimensional array computing; and\n",
    "*    [`pandas`](http://pandas.pydata.org/) for data analysis and manipulation.\n",
    "\n",
    "All packages used for this notebook can be obtained by downloading and installing the [Conda](https://anaconda.org/anaconda/conda) distribution, available on all platforms (Windows, Linux and Mac OSX). Step-by-step guides on how to install Anaconda can be found for Windows [here](https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444) and Mac [here](https://medium.com/@GalarnykMichael/install-python-on-mac-anaconda-ccd9f2014072), as well as in the Anaconda documentation itself [here](https://docs.anaconda.com/anaconda/install/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 (ideally)\n",
    "import platform\n",
    "import sys, getopt\n",
    "assert sys.version_info >= (3, 5)\n",
    "import csv\n",
    "\n",
    "# Import Dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "# Math Operations\n",
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "# Datetime\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "# Data Preprocessing\n",
    "import pandas as pd\n",
    "import pandas_profiling as pp\n",
    "import os\n",
    "import re\n",
    "import chardet\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "# Reading Directories\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Working with JSON\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "# Machine learning\n",
    "import scipy.signal as signal\n",
    "\n",
    "# Requests and downloads\n",
    "import tqdm\n",
    "import requests\n",
    "\n",
    "# Display in Jupyter\n",
    "from IPython.display import Image, YouTubeVideo\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "# Print message\n",
    "print('Setup Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.6\n",
      "NumPy: 1.19.1\n",
      "pandas: 1.1.3\n",
      "matplotlib: 3.3.1\n"
     ]
    }
   ],
   "source": [
    "# Python / module versions used here for reference\n",
    "print('Python: {}'.format(platform.python_version()))\n",
    "print('NumPy: {}'.format(np.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('matplotlib: {}'.format(mpl.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up initial paths to subfolders\n",
    "base_dir = os.path.join('..', '..')\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "data_dir_physical = os.path.join(base_dir, 'data', 'physical')\n",
    "scripts_dir = os.path.join(base_dir, 'scripts')\n",
    "models_dir = os.path.join(base_dir, 'models')\n",
    "img_dir = os.path.join(base_dir, 'img')\n",
    "fig_dir = os.path.join(base_dir, 'img', 'fig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns of displayed pandas DataFrames\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section2'></a>\n",
    "\n",
    "## <a id='#section2'>2. Notebook Brief</a>\n",
    "This notebook parses and engineers a provided dataset of physical data using [pandas](http://pandas.pydata.org/).\n",
    "\n",
    "\n",
    "**Notebook Conventions**:<br>\n",
    "*    Variables that refer a `DataFrame` object are prefixed with `df_`.\n",
    "*    Variables that refer to a collection of `DataFrame` objects (e.g., a list, a set or a dict) are prefixed with `dfs_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section3'></a>\n",
    "\n",
    "## <a id='#section3'>3. Data Sources</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.1'></a>\n",
    "\n",
    "### <a id='#section3.1'>3.1. Introduction</a>\n",
    "The physical data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.2'></a>\n",
    "\n",
    "### <a id='#section3.2'>3.2. Import Data</a>\n",
    "The `CSV` files provided will be read in as [pandas](https://pandas.pydata.org/) DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Read data directory\n",
    "print(glob.glob(os.path.join(data_dir_physical, 'raw', 'Set 2', '*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unify Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for unifying all the training data for a an indicated date into unified DataFrames\n",
    "def unify_training_data(date): \n",
    "\n",
    "    \"\"\"\n",
    "    Define a function to unify all the training data for a single data, defined in the function's parameter\n",
    "    of the formation 'YYYY-MM-DD'\n",
    "    \n",
    "    For this example dataset, there is data for just '2022-02-02'\n",
    "    \n",
    "    # KEY STEPS\n",
    "    # - USE GLOB TO PRODUCE SEPARATE DATAFRAMES FOR THE FOLLOWING:\n",
    "    ##  + ATTACK-VS-DEFENCE-ATTACK-SUPERIORITY\n",
    "    ##  + CROSSING-AND-FINISHING-HSR-SPR\n",
    "    ##  + FULL-SESSION-MODIFIED\n",
    "    ##  + MATCH-MSG\n",
    "    ##  + PASSING-DRILL-PHYSICAL\n",
    "    ##  + WARM-UP-COORDINATION-AGILITY\n",
    "    # - THESE UNIFIED DATAFRAMES NEED TO INCLUDE NEW COLUMNS FOR DATE (FILENAME) AND PLAYER NAME (FILENAME OR COLUMM)\n",
    "    # - AT THIS STAGE, THE UNIFIED DATAFRAMES CAN BE EXPORTED AS ENGINEERED FILES, BUT UNIFIED\n",
    "    # - NEXT, DROP ALL COLUMNS EXCEPT: Player Display Name, Time, Lat, Lon, Speed (m/s)\n",
    "    # - DEDUPLICATE THE DATAFRAME, MANY COLUMNS REMOVED ONCE GYRO DATA IGNORED\n",
    "    # - USE Player Display Name TO RENAME THE COLUMNS FOR Time, Lat, Lon, Speed (m/s), TO PREFIX WITH NAME\n",
    "    # - THEN DROP Player Display Name\n",
    "    # - USE LAURIE'S METRICA SCRIPT TO CALCULATE THE SPEED, DISTANCE, AND ACCELERATION USING THE LAT/LON AND TIMESTEP\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Read in exported CSV file if exists, if not, download the latest JSON data\n",
    "    if not os.path.exists(os.path.join(data_dir_physical, 'engineered', 'Set 2', f'{date}-ALL-TRAINING-DATA-ALL-PLAYERS.csv')):\n",
    "    \n",
    "        ### Start timer\n",
    "        tic = datetime.datetime.now()\n",
    "\n",
    "\n",
    "        ### Print time reading of CSV files started\n",
    "        print(f'Reading of CSV files started at: {tic}')\n",
    "\n",
    "\n",
    "        ### List all files available\n",
    "        lst_all_files = glob.glob(os.path.join(data_dir_physical, 'raw', 'Set 2', f'{date}-*.csv'))\n",
    "\n",
    "\n",
    "        ### Create an empty list to append individual DataFrames\n",
    "        lst_files_to_append =[]\n",
    "\n",
    "\n",
    "        ### Iterate through each file in list of all files\n",
    "        for file in lst_all_files:\n",
    "\n",
    "            ### Create temporary DataFrame with each file\n",
    "            df_temp = pd.read_csv(file, index_col=None, header=0)\n",
    "\n",
    "            ### Create a column that contains the filename - useful for information about the date, player, and training drill\n",
    "            df_temp['Filename'] = os.path.basename(file) \n",
    "\n",
    "            ### Append each individual Define each individual file to the empty list (to be concatenated) \n",
    "            lst_files_to_append.append(df_temp)\n",
    "\n",
    "\n",
    "        ### Concatenate all the files\n",
    "        df_all = pd.concat(lst_files_to_append, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "        ### Save DataFrame\n",
    "\n",
    "        #### Define filename for each combined file to be saved\n",
    "        save_filename = f'{date}-ALL-TRAINING-DATA-ALL-PLAYERS'.replace(' ', '-').replace('(', '').replace(')', '').replace(':', '').replace('.', '').replace('__', '_').upper()\n",
    "\n",
    "        #### Define the filepath to save each combined file\n",
    "        path = os.path.join(data_dir_physical, 'engineered', 'Set 2')\n",
    "\n",
    "        #### Save the combined file as a CSV\n",
    "        df_all.to_csv(path + f'/{save_filename}.csv', index=None, header=True)\n",
    "\n",
    "\n",
    "        ### Engineer the data\n",
    "\n",
    "        #### \n",
    "        df_all['Date'] = date\n",
    "\n",
    "        #### \n",
    "        #df_all['Training Type'] = training_type\n",
    "\n",
    "        #### Reorder Columns\n",
    "        #df_all = df_all[['Filename'] + [col for col in df_all.columns if col != 'Filename']]\n",
    "        #df_all = df_all[['Date'] + [col for col in df_all.columns if col != 'Date']]\n",
    "    \n",
    "    \n",
    "        ### End timer\n",
    "        toc = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "        ### Print time reading of CSV files end\n",
    "        print(f'Reading of CSV files ended at: {toc}')\n",
    "\n",
    "\n",
    "        ### Calculate time take\n",
    "        total_time = (toc-tic).total_seconds()\n",
    "        print(f'Time taken create a single DataFrame for from the individual CSV files is: {total_time/60:0.2f} minutes.')\n",
    "\n",
    "    \n",
    "    \n",
    "    ## If CSV file already exists, read in previously saved DataFrame\n",
    "    else:\n",
    "        \n",
    "        ### Print time reading of CSV files started\n",
    "        print('CSV file already saved to local storage. Reading in file as a pandas DataFrame.')\n",
    "        \n",
    "        ### Read in raw DataFrame\n",
    "        df_all = pd.read_csv(os.path.join(data_dir_physical, 'engineered', 'Set 2', f'{date}-ALL-TRAINING-DATA-ALL-PLAYERS.csv'))\n",
    "\n",
    "    \n",
    "    ## Return DataFrame\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file already saved to local storage. Reading in file as a pandas DataFrame.\n"
     ]
    }
   ],
   "source": [
    "df_training_data_all = unify_training_data('2022-02-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DataFrame\n",
    "df_training_data_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.3'></a>\n",
    "\n",
    "### <a id='#section3.3'>3.3. Initial Data Handling</a>\n",
    "First check the quality of the dataset by looking first and last rows in pandas using the [`head()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) and [`tail()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five rows of the DataFrame, df_training_data_all\n",
    "df_training_data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last five rows of the DataFrame, df_training_data_all\n",
    "df_training_data_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the DataFrame, df_training_data_all\n",
    "print(df_training_data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names of the DataFrame, df_training_data_all\n",
    "print(df_training_data_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types of the features of the DataFrame, df_training_data_all\n",
    "df_training_data_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full details of these attributes and their data types is discussed further in the [Data Dictionary](section3.2.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays all columns\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df_training_data_all.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info for the DataFrame, df_training_data_all\n",
    "df_training_data_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory usage is 2.7+ GB. The saved file is 4.2 GB, quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the raw DataFrame, df_training_data_all\n",
    "#msno.matrix(df_training_data_all, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of missing values\n",
    "null_value_stats = df_training_data_all.isnull().sum(axis=0)\n",
    "null_value_stats[null_value_stats != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section4'></a>\n",
    "\n",
    "## <a id='#section4'>4. Data Engineering</a>\n",
    "The next step is to wrangle the dataset to into a format that’s suitable for analysis and also to work with existing code to determine metrics such as speeds, distance, acceleration.\n",
    "\n",
    "This section is broken down into the following subsections:\n",
    "\n",
    "4.1.    [Prepare Training Data](#section4.1)<br>\n",
    "4.2.    [Split Out Unified Training Data into Individual Training Drills](#section4.2)<br>\n",
    "4.3.    [Engineer DataFrame to Match Tracking Data Format](#section4.3)<br>\n",
    "4.4.    [Calculate Speed, Distance, and Acceleration](#section4.4)<br>\n",
    "4.5.    [Create Physical Reports for Each Individual Training Session](#section4.5)<br>\n",
    "4.6.    [Create Single Physical Report for the Day of Interest](#section4.6)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4.1'></a>\n",
    "\n",
    "### <a id='#section4.1'>4.1. Prepare Training Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for unifying all the training data for a an indicated date into unified DataFrames\n",
    "def prepare_training_data(df, date): \n",
    "\n",
    "    \"\"\"\n",
    "    Define a function to prepare the unified training dataset'\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Read in exported CSV file if exists, if not, download the latest JSON data\n",
    "    if not os.path.exists(os.path.join(data_dir_physical, 'engineered', 'Set 2', f'{date}-ALL-MOVEMENT-TRAINING-DATA-ALL-PLAYERS.csv')):\n",
    "    \n",
    "    \n",
    "        ### Start timer\n",
    "        tic = datetime.datetime.now()\n",
    "\n",
    "        \n",
    "        ### Print time of engineering of tracking data started\n",
    "        print(f'Engineering of the unified training data CSV file started at: {tic}')\n",
    "        \n",
    "        \n",
    "        ### Select columns of interest and dedupe the DataFrame\n",
    "        df_select = df_training_data_all[['Player Display Name', 'Time', 'Lat', 'Lon', 'Speed (m/s)', 'Filename']].drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        ### Create Date column\n",
    "        df_select['Date'] = date\n",
    "        \n",
    "        \n",
    "        ### Convert Speed (m/s) to Speed (km/h)\n",
    "        df_select['Speed (km/h)'] = df_select['Speed (m/s)'] * 18/5\n",
    "        \n",
    "        \n",
    "        ### Use the Filename, Player Display Name and Date to determining the Training Drill\n",
    "        df_select['Training Drill'] = df_select['Filename']\n",
    "        df_select['Training Drill'] = df_select['Training Drill'].str.replace('JOAO-PEDRO', 'JOAO PEDRO')    # Temporary fix for Joao Pedro bug, fix later\n",
    "        df_select['Training Drill'] = df_select.apply(lambda x: x['Training Drill'].replace(x['Player Display Name'], ''), axis=1)\n",
    "        df_select['Training Drill'] = df_select.apply(lambda x: x['Training Drill'].replace(x['Date'], ''), axis=1)\n",
    "        df_select['Training Drill'] = df_select['Training Drill'].str.replace('--', '').str.replace('.csv', '')\n",
    "        \n",
    "        \n",
    "        ### Convert date from string type to  date type\n",
    "        df_select['Date'] = pd.to_datetime(df_select['Date'], errors='coerce', format='%Y-%m-%d')\n",
    "    \n",
    "        \n",
    "        ### Save DataFrame\n",
    "\n",
    "        #### Define filename for each combined file to be saved\n",
    "        save_filename = f'{date}-ALL-MOVEMENT-TRAINING-DATA-ALL-PLAYERS'.replace(' ', '-').replace('(', '').replace(')', '').replace(':', '').replace('.', '').replace('__', '_').upper()\n",
    "\n",
    "        #### Define the filepath to save each combined file\n",
    "        path = os.path.join(data_dir_physical, 'engineered', 'Set 2')\n",
    "\n",
    "        #### Save the combined file as a CSV\n",
    "        df_select.to_csv(path + f'/{save_filename}.csv', index=None, header=True)\n",
    "    \n",
    "    \n",
    "        ### End timer\n",
    "        toc = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "        ### Print time of engineering of tracking data ended\n",
    "        print(f'Engineering of the unified training data CSV file ended at: {toc}')\n",
    "\n",
    "\n",
    "        ### Calculate time take\n",
    "        total_time = (toc-tic).total_seconds()\n",
    "        print(f'Time taken to engineer and save unified training data is: {total_time/60:0.2f} minutes.')\n",
    "        \n",
    "    \n",
    "    \n",
    "    ## If CSV file already exists, read in previously saved DataFrame\n",
    "    else:\n",
    "        \n",
    "        ### Print time reading of CSV files started\n",
    "        print('Engineered CSV file of unified training already saved to local storage. Reading in file as a pandas DataFrame.')\n",
    "        \n",
    "        ### Read in raw DataFrame\n",
    "        df_select = pd.read_csv(os.path.join(data_dir_physical, 'engineered', 'Set 2', f'{date}-ALL-MOVEMENT-TRAINING-DATA-ALL-PLAYERS.csv'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Return DataFrame\n",
    "    return df_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_data_select = prepare_training_data(df_training_data_all, '2022-02-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_data_select "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_training_data_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_data_select.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_data_select.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statements about the dataset\n",
    "\n",
    "## Define variables for print statments\n",
    "training_drill_types = df_training_data_select['Training Drill'].unique()\n",
    "players = df_training_data_select['Player Display Name'].unique()\n",
    "count_training_drill_types = len(df_training_data_select['Training Drill'].unique())\n",
    "count_players = len(df_training_data_select['Player Display Name'].unique())\n",
    "\n",
    "## Print statements\n",
    "print(f'The Training DataFrame for 2022-02-02 contains the data for {count_training_drill_types:,} different training drills, including: {training_drill_types}.\\n')\n",
    "print(f'The Training DataFrame for 2022-02-02 contains the data for {count_players:,} different players, including: {players}.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4.2'></a>\n",
    "\n",
    "### <a id='#section4.2'>4.2. Split Out Unified Training Data into Individual Training Drills</a>\n",
    "Split out the unified DataFrame into the individual training drills.\n",
    "\n",
    "**Note**: It's important to do this before later conversions of the format and speed/acceleration calculations because not all the training sessions take place at the same time, as then they sessions could later get mixed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_training_types = list(df_training_data_select['Training Drill'].unique())\n",
    "lst_training_types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_match_msg = df_training_data_select[df_training_data_select['Training Drill'] == 'MATCH-MSG']\n",
    "df_training_crossing_and_finishing_hsr_spr = df_training_data_select[df_training_data_select['Training Drill'] == 'CROSSING-AND-FINISHING-HSR-SPR']\n",
    "df_training_attack_vs_defence_attack_superiority = df_training_data_select[df_training_data_select['Training Drill'] == 'ATTACK-VS-DEFENCE-ATTACK-SUPERIORITY']\n",
    "df_training_full_session_modified = df_training_data_select[df_training_data_select['Training Drill'] == 'FULL-SESSION-MODIFIED']\n",
    "df_training_passing_drill_physical = df_training_data_select[df_training_data_select['Training Drill'] == 'PASSING-DRILL-PHYSICAL']\n",
    "df_training_warm_up_coordination_agility = df_training_data_select[df_training_data_select['Training Drill'] == 'WARM-UP-COORDINATION-AGILITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_match_msg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_match_msg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_training_match_msg['Player Display Name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4.3'></a>\n",
    "\n",
    "### <a id='#section4.3'>4.3. Engineer DataFrame to Match Tracking Data Format</a>\n",
    "To work with the existing Tracking data libraries, based on [Laurie Shaw](https://twitter.com/EightyFivePoint)'s Metrica Sports Tracking data libraries, [`LaurieOnTracking`](https://github.com/Friends-of-Tracking-Data-FoTD/LaurieOnTracking), the data needs to be engineered to match the Metrica schema, which is the following:\n",
    "\n",
    "| Feature                                   | Data type     | Definition     |\n",
    "|-------------------------------------------|---------------|----------------|\n",
    "| `Frame`                                   | int64         |                |\n",
    "| `Period`                                  | int64         |                |\n",
    "| `Time [s]`                                | float64       |                |\n",
    "| `Home/Away_No._x` (repeated 14 times)     | float64       |                |\t\n",
    "| `Home/Away_No._y` (repeated 14 times)     | float64       |                |\n",
    "| `ball_x`                                  | float64       |\t             |\n",
    "| `ball_y`                                  | float64       |                |\n",
    "\n",
    "To learn more about the Metrica Sports schema, see the official documentation [[link](https://github.com/metrica-sports/sample-data/blob/master/documentation/events-definitions.pdf)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_match_msg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for unifying all the training data for a an indicated date into unified DataFrames\n",
    "def convert_training_data_format(df, date, training_drill): \n",
    "\n",
    "    \"\"\"\n",
    "    Define a function to convert the format of the training dataset to match Tracking data'\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Read in exported CSV file if exists, if not, download the latest JSON data\n",
    "    if not os.path.exists(os.path.join(data_dir_physical, 'engineered', 'Set 2', f'{date}-{training_drill}-MOVEMENT-TRAINING-DATA-ALL-PLAYERS.csv')):\n",
    "    \n",
    "    \n",
    "        ### Start timer\n",
    "        tic = datetime.datetime.now()\n",
    "\n",
    "        \n",
    "        ### Print time of engineering of tracking data started\n",
    "        print(f'Conversion of the format of the training data started at: {tic}')        \n",
    "        \n",
    "        \n",
    "        ##\n",
    "        df_pvt = df.copy()\n",
    "\n",
    "\n",
    "        ##\n",
    "        lst_players = list(df_pvt['Player Display Name'].unique())\n",
    "\n",
    "\n",
    "        ## Rename columns\n",
    "        df_pvt = df_pvt.rename(columns={'Time': 'Time [s]',\n",
    "                                        'Lon': 'x',\n",
    "                                        'Lat': 'y'\n",
    "                                       }\n",
    "                              )\n",
    "\n",
    "\n",
    "        ##\n",
    "        df_pvt = df_pvt.drop(columns=['Filename'])\n",
    "\n",
    "\n",
    "        ## Create empty DataFrame of timestamps\n",
    "        df_time = df_pvt[['Time [s]', 'Date', 'Training Drill']].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "        ## Create empty DataFrame of timestamps\n",
    "        df_time = df_time.reset_index(drop=False)\n",
    "        \n",
    "        \n",
    "        ## Rename index column to 'Frame'\n",
    "        df_time = df_time.rename(columns={'index': 'Frame'})\n",
    "        \n",
    "        \n",
    "        ##\n",
    "        df_pvt_final = df_time.copy()\n",
    "\n",
    "\n",
    "        ## Iterate through each file in list of all files\n",
    "        for player in lst_players:\n",
    "\n",
    "            ### Create temporary DataFrame with each file\n",
    "            df_player = df_pvt[df_pvt['Player Display Name'] == player]\n",
    "\n",
    "            ###\n",
    "            df_player['Player'] = df_player['Player Display Name'].str.title()\n",
    "\n",
    "            ### \n",
    "            player_title = player.title()\n",
    "\n",
    "            ###\n",
    "            df_player = df_player.rename(columns={'Time [s]': 'Time',\n",
    "                                                  'x': f'{player_title}_x',\n",
    "                                                  'y': f'{player_title}_y',\n",
    "                                                  'Speed (m/s)': f'{player_title} Speed (m/s)',\n",
    "                                                  'Speed (km/h)': f'{player_title} Speed (km/h)'\n",
    "                                                 }\n",
    "                                        )\n",
    "\n",
    "\n",
    "            ### \n",
    "            df_player = df_player[['Time', f'{player_title}_x', f'{player_title}_y', f'{player_title} Speed (m/s)', f'{player_title} Speed (km/h)']]\n",
    "\n",
    "\n",
    "            ### Join each individual DataFrame to time DataFrame\n",
    "            df_pvt_final = pd.merge(df_pvt_final, df_player, left_on=['Time [s]'], right_on=['Time'], how='left')\n",
    "\n",
    "            ### \n",
    "            df_pvt_final = df_pvt_final.drop(columns=['Time'])\n",
    "\n",
    "            ### \n",
    "            df_pvt_final = df_pvt_final.drop_duplicates()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Save DataFrame\n",
    "\n",
    "        #### Define filename for each combined file to be saved\n",
    "        save_filename = f'{date}-{training_drill}-MOVEMENT-TRAINING-DATA-ALL-PLAYERS'.replace(' ', '-').replace('(', '').replace(')', '').replace(':', '').replace('.', '').replace('__', '_').upper()\n",
    "\n",
    "        #### Define the filepath to save each combined file\n",
    "        path = os.path.join(data_dir_physical, 'engineered', 'Set 2')\n",
    "\n",
    "        #### Save the combined file as a CSV\n",
    "        df_pvt_final.to_csv(path + f'/{save_filename}.csv', index=None, header=True)\n",
    "        \n",
    "        \n",
    "        ### End timer\n",
    "        toc = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "        ### Print time of engineering of tracking data ended\n",
    "        print(f'Conversion of the format of the training data ended at: {toc}')\n",
    "\n",
    "\n",
    "        ### Calculate time take\n",
    "        total_time = (toc-tic).total_seconds()\n",
    "        print(f'Time taken to convert the format and save the training data is: {total_time:0.2f} seconds.')\n",
    "        \n",
    "    \n",
    "    \n",
    "    ## If CSV file already exists, read in previously saved DataFrame\n",
    "    else:\n",
    "        \n",
    "        ### Print time reading of CSV files started\n",
    "        print('Converted training data already saved to local storage. Reading in file as a pandas DataFrame.')\n",
    "        \n",
    "        ### Read in raw DataFrame\n",
    "        df_pvt_final = pd.read_csv(os.path.join(data_dir_physical, 'engineered', 'Set 2', f'{date}-{training_drill}-MOVEMENT-TRAINING-DATA-ALL-PLAYERS.csv'))\n",
    "        \n",
    "        \n",
    "    ## Return the DataFrame\n",
    "    return(df_pvt_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_match_msg_pvt = convert_training_data_format(df=df_training_match_msg, date='2022-02-02', training_drill='MATCH-MSG')\n",
    "df_training_crossing_and_finishing_hsr_spr_pvt = convert_training_data_format(df=df_training_crossing_and_finishing_hsr_spr, date='2022-02-02', training_drill='CROSSING-AND-FINISHING-HSR-SPR')\n",
    "df_training_attack_vs_defence_attack_superiority_pvt = convert_training_data_format(df=df_training_attack_vs_defence_attack_superiority, date='2022-02-02', training_drill='ATTACK-VS-DEFENCE-ATTACK-SUPERIORITY')\n",
    "df_training_full_session_modified_pvt = convert_training_data_format(df=df_training_full_session_modified, date='2022-02-02', training_drill='FULL-SESSION-MODIFIED')\n",
    "df_training_passing_drill_physical_pvt = convert_training_data_format(df=df_training_passing_drill_physical, date='2022-02-02', training_drill='PASSING-DRILL-PHYSICAL')\n",
    "df_training_warm_up_coordination_agility_pvt = convert_training_data_format(df=df_training_warm_up_coordination_agility, date='2022-02-02', training_drill='WARM-UP-COORDINATION-AGILITY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the DataFrame, df_training_match_msg_pvt\n",
    "msno.matrix(df_training_match_msg_pvt, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the DataFrame, df_training_crossing_and_finishing_hsr_spr_pvt\n",
    "msno.matrix(df_training_crossing_and_finishing_hsr_spr_pvt, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the DataFrame, df_training_attack_vs_defence_attack_superiority_pvt\n",
    "msno.matrix(df_training_attack_vs_defence_attack_superiority_pvt, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the DataFrame, df_training_full_session_modified_pvt\n",
    "msno.matrix(df_training_full_session_modified_pvt, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the DataFrame, df_training_passing_drill_physical_pvt\n",
    "msno.matrix(df_training_passing_drill_physical_pvt, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the DataFrame, df_training_warm_up_coordination_agility_pvt\n",
    "msno.matrix(df_training_warm_up_coordination_agility_pvt, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualisation, we can see that, that for certain drills, all the players are involved. However, for some drills the players are involved at different times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_training_attack_vs_defence_attack_superiority_pvt.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4.4'></a>\n",
    "\n",
    "### <a id='#section4.4'>4.4. Calculate Speed, Distance, and Acceleration</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for calculating the velocities and accelerations of the training data using the x, y locations and timestep\n",
    "def calc_player_velocities_accelerations(df, date='2022-02-02', training_drill='NOT-DEFINED', smoothing_v=True, smoothing_a=True, filter_='moving_average', window=7, polyorder=1, maxspeed=12, dt=0.01):\n",
    "    \"\"\" calc_player_velocities_accelerations( training data )\n",
    "    \n",
    "    Calculate player velocities in x & y direction, and total player speed at each timestamp of the tracking data\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "        df: the tracking DataFrame\n",
    "        smoothing_v: boolean variable that determines whether velocity measures are smoothed. Default is True.\n",
    "        filter: type of filter to use when smoothing_v the velocities. Default is Savitzky-Golay, which fits a polynomial of order 'polyorder' to the data within each window\n",
    "        window: smoothing_v window size in # of frames\n",
    "        polyorder: order of the polynomial for the Savitzky-Golay filter. Default is 1 - a linear fit to the velcoity, so gradient is the acceleration\n",
    "        maxspeed: the maximum speed that a player can realisitically achieve (in meters/second). Speed measures that exceed maxspeed are tagged as outliers and set to NaN. \n",
    "        \n",
    "    Returns\n",
    "    -----------\n",
    "       df : the tracking DataFrame with columns for speed in the x & y direction and total speed added\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Read in exported CSV file if exists, if not, download the latest JSON data\n",
    "    if not os.path.exists(os.path.join(data_dir_physical, 'engineered', 'Set 2', f'{date}-{training_drill}-MOVEMENT-SPEED-ACCELERATION-TRAINING-DATA-ALL-PLAYERS.csv')):\n",
    "    \n",
    "    \n",
    "        ### Start timer\n",
    "        tic = datetime.datetime.now()\n",
    "\n",
    "        \n",
    "        ### Print time of engineering of tracking data started\n",
    "        print(f'Calculation of each player\\'s speed and accelerations for the {training_drill} started at: {tic}')        \n",
    "        \n",
    "        \n",
    "        # Create columns\n",
    "       #df['Date Time [s]'] = pd.to_datetime(df['Date'] + ' ' + df['Time [s]'])\n",
    "        df['Period'] = 1\n",
    "\n",
    "        # remove any velocity data already in the dataframe\n",
    "        df = remove_player_velocities_accelerations(df)\n",
    "\n",
    "        # Get the player ids\n",
    "        #player_ids = np.unique( [ c[:-2] for c in df.columns if c[:4] in ['Home','Away'] ] )\n",
    "        player_ids = [col for col in df.columns if '_x' in col]\n",
    "        player_ids = [s.replace('_x', '') for s in player_ids]\n",
    "\n",
    "        # Calculate the timestep from one frame to the next - not required.\n",
    "        #dt = df['Time [s]'].diff()\n",
    "        #dt = df['Date Time [s]'].diff()\n",
    "\n",
    "        # index of first frame in second half\n",
    "        #second_half_idx = df.Period.idxmax(2)\n",
    "        second_half_idx = df[df.Period == 2].first_valid_index()    # replace Laurie's function with one that works better\n",
    "\n",
    "        # estimate velocities for players in df\n",
    "        for player in player_ids: # cycle through players individually\n",
    "            # difference player positions in timestep dt to get unsmoothed estimate of velicity\n",
    "            vx = df[player + '_x'].diff() / dt\n",
    "            vy = df[player + '_y'].diff() / dt\n",
    "\n",
    "            if maxspeed>0:\n",
    "                # remove unsmoothed data points that exceed the maximum speed (these are most likely position errors)\n",
    "                raw_speed = np.sqrt( vx**2 + vy**2 )\n",
    "               #acceleration = raw_speed.diff() / dt\n",
    "                vx[ raw_speed>maxspeed ] = np.nan\n",
    "                vy[ raw_speed>maxspeed ] = np.nan\n",
    "           #if maxacc>0:\n",
    "               #ax[ raw_acc>maxacc ] = np.nan\n",
    "               #ay[ raw_acc>maxacc ] = np.nan\n",
    "            if smoothing_v:\n",
    "                if filter_=='Savitzky-Golay':\n",
    "                    # calculate first half velocity\n",
    "                    vx.loc[:second_half_idx] = signal.savgol_filter(vx.loc[:second_half_idx],window_length=window,polyorder=polyorder)\n",
    "                    vy.loc[:second_half_idx] = signal.savgol_filter(vy.loc[:second_half_idx],window_length=window,polyorder=polyorder)        \n",
    "                    # calculate second half velocity\n",
    "                    vx.loc[second_half_idx:] = signal.savgol_filter(vx.loc[second_half_idx:],window_length=window,polyorder=polyorder)\n",
    "                    vy.loc[second_half_idx:] = signal.savgol_filter(vy.loc[second_half_idx:],window_length=window,polyorder=polyorder)\n",
    "                elif filter_=='moving average':\n",
    "                    ma_window = np.ones( window ) / window \n",
    "                    # calculate first half velocity\n",
    "                    vx.loc[:second_half_idx] = np.convolve( vx.loc[:second_half_idx], ma_window, mode='same') \n",
    "                    vy.loc[:second_half_idx] = np.convolve( vy.loc[:second_half_idx], ma_window, mode='same')      \n",
    "                    # calculate second half velocity\n",
    "                    vx.loc[second_half_idx:] = np.convolve( vx.loc[second_half_idx:], ma_window, mode='same') \n",
    "                    vy.loc[second_half_idx:] = np.convolve( vy.loc[second_half_idx:], ma_window, mode='same') \n",
    "                   #speed = ( vx**2 + vy**2 )**.5\n",
    "                   #acceleration = np.diff(speed) / dt\n",
    "                   #ax = np.convolve( ax, ma_window, mode='same' ) \n",
    "                   #ay = np.convolve( ay, ma_window, mode='same' )              \n",
    "                   # put player speed in x, y direction, and total speed back in the data frame\n",
    "\n",
    "            # put player speed in x, y direction, and total speed back in the data frame\n",
    "            df[player + '_vx'] = vx\n",
    "            df[player + '_vy'] = vy\n",
    "            df[player + '_speed'] = np.sqrt(vx**2 + vy**2)\n",
    "           #df[player + '_ax'] = ax\n",
    "           #df[player + '_ay'] = ay\n",
    "           #df[player + '_rawspeed'] = raw_speed\n",
    "           #df[player + '_rawacc'] = raw_acc\n",
    "            df[player + '_speed'] = np.sqrt(vx**2 + vy**2)\n",
    "            # Calculate acceleration - method 1, using speed calculated\n",
    "            acceleration = df[player + '_speed'].diff() / dt\n",
    "            df[player + '_acceleration'] = acceleration\n",
    "            # Calculate acceleration - method 2, using speed provided\n",
    "            acceleration = df[player + ' Speed (m/s)'].diff() / dt\n",
    "            df[player + ' Acceleration (m/s/s)'] = acceleration \n",
    "            if smoothing_a:\n",
    "                ma_window = np.ones( window ) / window \n",
    "                df[player + ' Acceleration (m/s/s)'] = np.convolve( acceleration, ma_window, mode='same')  \n",
    "                \n",
    "        \n",
    "        ### Save DataFrame\n",
    "\n",
    "        #### Define filename for each combined file to be saved\n",
    "        save_filename = f'{date}-{training_drill}-MOVEMENT-SPEED-ACCELERATION-TRAINING-DATA-ALL-PLAYERS'.replace(' ', '-').replace('(', '').replace(')', '').replace(':', '').replace('.', '').replace('__', '_').upper()\n",
    "\n",
    "        #### Define the filepath to save each combined file\n",
    "        path = os.path.join(data_dir_physical, 'engineered', 'Set 2')\n",
    "\n",
    "        #### Save the combined file as a CSV\n",
    "        df.to_csv(path + f'/{save_filename}.csv', index=None, header=True)\n",
    "        \n",
    "        \n",
    "        ### End timer\n",
    "        toc = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "        ### Print time of engineering of tracking data ended\n",
    "        print(f'Calculation of each player\\'s speed and accelerations for the {training_drill} ended at: {toc}')\n",
    "\n",
    "\n",
    "        ### Calculate time take\n",
    "        total_time = (toc-tic).total_seconds()\n",
    "        print(f'Time taken to calculate speed and acceleration and save the training data is: {total_time:0.2f} seconds.')\n",
    "        \n",
    "    \n",
    "    \n",
    "    ## If CSV file already exists, read in previously saved DataFrame\n",
    "    else:\n",
    "        \n",
    "        ### Print time reading of CSV files started\n",
    "        print('Training data with calculated velocities and accelerations already saved to local storage. Reading in file as a pandas DataFrame.')\n",
    "        \n",
    "        ### Read in raw DataFrame\n",
    "        df = pd.read_csv(os.path.join(data_dir_physical, 'engineered', 'Set 2', f'{date}-{training_drill}-MOVEMENT-SPEED-ACCELERATION-TRAINING-DATA-ALL-PLAYERS.csv'))\n",
    "        \n",
    "        \n",
    "    ## Return the DataFrame\n",
    "    return(df)\n",
    "\n",
    "def remove_player_velocities_accelerations(df):\n",
    "    # remove player velocities and acceleration measures that are already in the 'df' dataframe\n",
    "    columns = [c for c in df.columns if c.split('_')[-1] in ['vx', 'vy', 'ax', 'ay', 'rawspeed', 'rawacc', 'speed', 'acceleration']]    # Get the player ids\n",
    "    df = df.drop(columns=columns)\n",
    "    return df\n",
    "\n",
    "def compute_accelaration(df):\n",
    "    \"\"\"\n",
    "    Function to determine's player accelaration from the tracking data and estimates the maximum rate of acceleration for each player\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the player ids\n",
    "    v_columns = [i for i in df.columns if '_speed' in i]\n",
    "\n",
    "    # Calculate the timestep from one frame to the next. Should always be 0.01 within the same half\n",
    "    dt = df['Time [s]'].diff()\n",
    "    \n",
    "\n",
    "    for c in v_columns:\n",
    "        player = '_'.join(c.split('_')[:2])\n",
    "        \n",
    "        ax = df[player + '_vx'].diff() / dt\n",
    "        ay = df[player + '_vy'].diff() / dt\n",
    "        \n",
    "        # put player speed in x, y direction, and total speed back in the data frame\n",
    "        df[player + '_ax'] = ax\n",
    "        df[player + '_ay'] = ay\n",
    "        df[player + '_acc'] = np.sqrt( ax**2 + ay**2 )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the velocity and accelerations for each player in each of the six training sessions\n",
    "df_training_match_msg_vel = calc_player_velocities_accelerations(df=df_training_match_msg_pvt, date='2022-02-02', training_drill='MATCH-MSG', smoothing_v=True, smoothing_a=True, filter_='moving_average', window=7, polyorder=1, maxspeed=12, dt=0.01)\n",
    "df_training_crossing_and_finishing_hsr_spr_vel = calc_player_velocities_accelerations(df=df_training_crossing_and_finishing_hsr_spr_pvt, date='2022-02-02', training_drill='CROSSING-AND-FINISHING-HSR-SPR', smoothing_v=True, smoothing_a=True, filter_='moving_average', window=7, polyorder=1, maxspeed=12, dt=0.01)\n",
    "df_training_attack_vs_defence_attack_superiority_vel = calc_player_velocities_accelerations(df=df_training_attack_vs_defence_attack_superiority_pvt, date='2022-02-02', training_drill='ATTACK-VS-DEFENCE-ATTACK-SUPERIORITY', smoothing_v=True, smoothing_a=True, filter_='moving_average', window=7, polyorder=1, maxspeed=12, dt=0.01)\n",
    "df_training_full_session_modified_vel = calc_player_velocities_accelerations(df=df_training_full_session_modified_pvt, date='2022-02-02', training_drill='FULL-SESSION-MODIFIED', smoothing_v=True, smoothing_a=True, filter_='moving_average', window=7, polyorder=1, maxspeed=12, dt=0.01)\n",
    "df_training_passing_drill_physical_vel = calc_player_velocities_accelerations(df=df_training_passing_drill_physical_pvt, date='2022-02-02', training_drill='PASSING-DRILL-PHYSICAL', smoothing_v=True, smoothing_a=True, filter_='moving_average', window=7, polyorder=1, maxspeed=12, dt=0.01)\n",
    "df_training_warm_up_coordination_agility_vel = calc_player_velocities_accelerations(df=df_training_warm_up_coordination_agility_pvt, date='2022-02-02', training_drill='WARM-UP-COORDINATION-AGILITY', smoothing_v=True, smoothing_a=True, filter_='moving_average', window=7, polyorder=1, maxspeed=12, dt=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_training_attack_vs_defence_attack_superiority_vel.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_attack_vs_defence_attack_superiority_vel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4.5'></a>\n",
    "\n",
    "### <a id='#section4.5'>4.5. Create Physical Reports for Each Individual Training Session</a>\n",
    "Arbitrary speed zones defined as:\n",
    "*    Low-Speed Activities (LSA) (<14 km/h or <4 m/s);\n",
    "*    Moderate-Speed Running (MSR) (14.4–19.8 km/h or 4-5.5 m/s);\n",
    "*    High-Speed Running (HSR) (19.8–25.1 km/h or 5.5-6.972 m/s); and\n",
    "*    Sprinting (≥25.2 km km/h or ≥6.972 m/s).\n",
    "\n",
    "For further information, see: [Application of Individualized Speed Zones to Quantify External Training Load in Professional Soccer](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7126260/) by Vincenzo Rago, João Brito, Pedro Figueiredo, Peter Krustrup, and António Rebelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate a bespoke physical summary of all the players for an individual training session\n",
    "def create_physical_report_per_training_session(df, date='2022-02-02', training_drill='NOT-DEFINED'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Define a function to generate a bespoke physical summary of all the players for an individual training session\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Read in exported CSV file if exists, if not, download the latest JSON data\n",
    "    if not os.path.exists(os.path.join(data_dir_physical, 'engineered', 'Set 2', f'{date}-{training_drill}-PHYSICAL-REPORT-ALL-PLAYERS.csv')):\n",
    "    \n",
    "    \n",
    "        ### Start timer\n",
    "        tic = datetime.datetime.now()\n",
    "\n",
    "        \n",
    "        ### Print time of engineering of tracking data started\n",
    "        print(f'Creation of the physical report for the {training_drill} training drill started at: {tic}')        \n",
    "    \n",
    "    \n",
    "        ## Data Engineering\n",
    "\n",
    "        ### \n",
    "        lst_cols = list(df)\n",
    "\n",
    "        ###\n",
    "        lst_players = []\n",
    "\n",
    "        ###\n",
    "        for col in lst_cols:\n",
    "            if '_x' in col:\n",
    "                col = col.replace('_x', '')\n",
    "                lst_players.append(col)\n",
    "\n",
    "        ### Create DataFrame where each row is a player\n",
    "        df_summary = pd.DataFrame(lst_players, columns=['Player'])\n",
    "\n",
    "        \n",
    "    \n",
    "        ##\n",
    "        df_summary['Date'] = date        \n",
    "        df_summary['Training Drill'] = training_drill\n",
    "    \n",
    "\n",
    "    \n",
    "        ## Calculate minutes played for each player\n",
    "\n",
    "        ### Create empty list for minutes\n",
    "        lst_minutes = []\n",
    "\n",
    "        ### Cycle through each player's jersey number in the team and look for the first and last time for each player\n",
    "        for player in lst_players:\n",
    "\n",
    "            #### Search for first and last frames that we have a position observation for each player (when a player is not on the pitch positions are NaN)\n",
    "            column = f'{player}' + '_x'     # use player x-position coordinate\n",
    "            try:\n",
    "                player_minutes = (df[column].last_valid_index() - df[column].first_valid_index() + 1) / 10 / 60     # convert to minutes\n",
    "            except:\n",
    "                player_minutes = 0\n",
    "            lst_minutes.append(player_minutes)\n",
    "\n",
    "        ### Create column for the minute played\n",
    "        df_summary['Minutes Trained'] = lst_minutes\n",
    "\n",
    "        ### Sort values by minutes played descending\n",
    "        df_summary = df_summary.sort_values(['Minutes Trained'], ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "        ## Calculate total distance covered for each player\n",
    "\n",
    "        ### Create empty list for distance\n",
    "        lst_distance = []\n",
    "\n",
    "        ### Cycle through each player and multiple their speed at any given instance by 10ms to get total distance and divide by 1,000 to get this in km\n",
    "        for player in lst_players:\n",
    "            column = f'{player}' + ' Speed (m/s)'\n",
    "            df_player_distance = df[column].sum()/100./1000          # speed time. Convert to km (original logic)\n",
    "           #df_player_distance = (df[column].sum() * 0.01) / 1000    # Distance = Speed * Time\n",
    "            lst_distance.append(df_player_distance)\n",
    "\n",
    "        ### Create column for the distance in km\n",
    "        df_summary['Distance [km]'] = lst_distance\n",
    "\n",
    "        \n",
    "\n",
    "        ## Calculate total distance covered for each player for different types of movement\n",
    "\n",
    "        ### Create empty lists for distances of different movements\n",
    "        lst_lsa = []\n",
    "        lst_msr = []\n",
    "        lst_hsr = []\n",
    "        lst_sprinting = []\n",
    "\n",
    "        ### Cycle through each player's jersey number in the team and \n",
    "        for player in lst_players:\n",
    "            column = f'{player}' + ' Speed (m/s)'\n",
    "            ### Low-Speed Activities (LSA) (<14 km/h or <4 m/s)\n",
    "            player_distance = df.loc[df[column] < 4, column].sum()/100./1000\n",
    "           #player_distance = df.loc[df[column] < 14.4, column].sum()/100./1000\n",
    "            lst_lsa.append(player_distance)\n",
    "            ### Moderate-Speed Running (MSR) (14.4–19.8 km/h or 4-5.5 m/s)\n",
    "            player_distance = df.loc[(df[column] >= 4) & (df[column] < 5.5), column].sum()/100./1000\n",
    "           #player_distance = df.loc[(df[column] >= 14.4) & (df[column] < 19.8), column].sum()/100./1000\n",
    "            lst_msr.append(player_distance)\n",
    "            ### High-Speed Running (HSR) (19.8–25.1 km/h or 5.5-6.972 m/s)\n",
    "            player_distance = df.loc[(df[column] >= 5.5) & (df[column] < 6.972222), column].sum()/100./1000\n",
    "           #player_distance = df.loc[(df[column] >= 19.8) & (df[column] < 25.1), column].sum()/100./1000\n",
    "            lst_hsr.append(player_distance)\n",
    "            ### Sprinting (≥25.2 km km/h or ≥6.972 m/s)\n",
    "            player_distance = df.loc[df[column] >= 6.972222, column].sum()/100./1000\n",
    "           #player_distance = df.loc[df[column] >= 25.2, column].sum()/100./1000\n",
    "            lst_sprinting.append(player_distance)\n",
    "\n",
    "        ### Assign each movement list to a column in the Summary DataFrame\n",
    "        df_summary['Low-Speed Activities (LSA) [km]'] = lst_lsa\n",
    "        df_summary['Moderate-Speed Running (MSR) [km]'] = lst_msr\n",
    "        df_summary['High-Speed Running (HSR) [km]'] = lst_hsr\n",
    "        df_summary['Sprinting [km]'] = lst_sprinting\n",
    "\n",
    "        \n",
    "        \n",
    "        ## Reset index\n",
    "        df_summary = df_summary.reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        ## Determine the number of sustained sprints per match\n",
    "\n",
    "        ### Create an empty list for the number of sprints\n",
    "        nsprints = []\n",
    "\n",
    "        ###\n",
    "       #sprint_threshold = 25.2        # minimum speed to be defined as a sprint (km/h)\n",
    "        sprint_threshold = 6.972222    # minimum speed to be defined as a sprint (m/s)\n",
    "        sprint_window = 1 * 10\n",
    "\n",
    "        ###\n",
    "        for player in lst_players:\n",
    "            column = f'{player}' + ' Speed (m/s)'\n",
    "            # trick here is to convolve speed with a window of size 'sprint_window', and find number of occassions that sprint was sustained for at least one window length\n",
    "            # diff helps us to identify when the window starts\n",
    "            player_sprints = np.diff(1 * (np.convolve(1 * (df[column] >= sprint_threshold), np.ones(sprint_window), mode='same') >= sprint_window))\n",
    "            nsprints.append(np.sum(player_sprints == 1 ))\n",
    "\n",
    "        ### Add column for the number of sprints\n",
    "        df_summary['No. Sprints'] = nsprints\n",
    "\n",
    "\n",
    "        \n",
    "        ## Estimate the top speed of each player\n",
    "\n",
    "        ### Create empty dictionary to append maximum speeds\n",
    "        dict_top_speeds = {}\n",
    "\n",
    "        ### Iterate through the columns of the training DataFrame for the top speeds\n",
    "        player_speed_columns = [i for i in df.columns if ' Speed (m/s)' in i]\n",
    "\n",
    "        ### Iterate through all the rows of all the speed columns, to determine the maximum speed for each player\n",
    "        for player in player_speed_columns:\n",
    "            dict_top_speeds[player] = df[player].max()\n",
    "\n",
    "        ### \n",
    "        df_top_speeds = pd.DataFrame.from_dict(dict_top_speeds, orient='index', columns=['Top Speed [m/s]'])\n",
    "\n",
    "        ### \n",
    "        df_top_speeds = df_top_speeds.reset_index(drop=False)\n",
    "\n",
    "        ### \n",
    "        df_top_speeds = df_top_speeds.rename(columns={'index': 'Player'})\n",
    "\n",
    "        ### \n",
    "        df_top_speeds['Player'] = df_top_speeds['Player'].str.replace(' Speed (m/s)', '')\n",
    "\n",
    "        ### \n",
    "        df_top_speeds['Player'] = df_top_speeds['Player'].str.replace(' Speed \\(m/s\\)', '')\n",
    "\n",
    "        ### Merge Top Speeds DataFrame to Summary DataFrame\n",
    "        df_summary = pd.merge(df_summary, df_top_speeds, left_on=['Player'], right_on=['Player'], how='left')\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Estimate the top acceleration of each player\n",
    "\n",
    "        ### Create empty dictionary to append maximum accelerations\n",
    "        dict_top_accelerations = {}\n",
    "\n",
    "        ### Iterate through the columns of the training DataFrame for the top accelerations\n",
    "        player_acceleration_columns = [i for i in df.columns if ' Acceleration (m/s/s)' in i]\n",
    "\n",
    "        ### Iterate through all the rows of all the acceleration columns, to determine the maximum acceleration for each player\n",
    "        for player in player_acceleration_columns:\n",
    "            dict_top_accelerations[player] = df[player].max()\n",
    "\n",
    "        ### \n",
    "        df_top_accelerations = pd.DataFrame.from_dict(dict_top_accelerations, orient='index', columns=['Top Acceleration [m/s/s]'])\n",
    "\n",
    "        ### \n",
    "        df_top_accelerations = df_top_accelerations.reset_index(drop=False)\n",
    "\n",
    "        ### \n",
    "        df_top_accelerations = df_top_accelerations.rename(columns={'index': 'Player'})\n",
    "\n",
    "        ### \n",
    "        df_top_accelerations['Player'] = df_top_accelerations['Player'].str.replace('_acceleration', '')\n",
    "\n",
    "        ### \n",
    "        df_top_accelerations['Player'] = df_top_accelerations['Player'].str.replace('_acceleration', '')\n",
    "        \n",
    "        ### Merge Top Speeds DataFrame to Summary DataFrame\n",
    "        df_summary = pd.merge(df_summary, df_top_accelerations, left_on=['Player'], right_on=['Player'], how='left')\n",
    "        \n",
    "\n",
    "        ### Save DataFrame\n",
    "\n",
    "        #### Define filename for each combined file to be saved\n",
    "        save_filename = f'{date}-{training_drill}-PHYSICAL-REPORT-ALL-PLAYERS'.replace(' ', '-').replace('(', '').replace(')', '').replace(':', '').replace('.', '').replace('__', '_').upper()\n",
    "\n",
    "        #### Define the filepath to save each combined file\n",
    "        path = os.path.join(data_dir_physical, 'engineered', 'Set 2')\n",
    "\n",
    "        #### Save the combined file as a CSV\n",
    "        df_summary.to_csv(path + f'/{save_filename}.csv', index=None, header=True)\n",
    "        \n",
    "        \n",
    "        ### End timer\n",
    "        toc = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "        ### Print time of engineering of tracking data ended\n",
    "        print(f'Creation of the physical report for the {training_drill} training drill ended at: {toc}')\n",
    "\n",
    "\n",
    "        ### Calculate time take\n",
    "        total_time = (toc-tic).total_seconds()\n",
    "        print(f'Time taken to create the physical report for the {training_drill} training data is: {total_time:0.2f} seconds.')\n",
    "        \n",
    "    \n",
    "    \n",
    "    ## If CSV file already exists, read in previously saved DataFrame\n",
    "    else:\n",
    "        \n",
    "        ### Print time reading of CSV files started\n",
    "        print('Physical report already saved to local storage. Reading in file as a pandas DataFrame.')\n",
    "        \n",
    "        ### Read in raw DataFrame\n",
    "        df_summary = pd.read_csv(os.path.join(data_dir_physical, 'engineered', 'Set 2', f'{date}-{training_drill}-PHYSICAL-REPORT-ALL-PLAYERS.csv'))\n",
    "    \n",
    "    \n",
    "    ## Return DataFrame\n",
    "    return df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create physical reports for each player in each of the six training sessions\n",
    "df_training_match_msg_physical_report = create_physical_report_per_training_session(df_training_match_msg_vel, date='2022-02-02', training_drill='MATCH-MSG')\n",
    "df_training_crossing_and_finishing_hsr_spr_physical_report = create_physical_report_per_training_session(df_training_crossing_and_finishing_hsr_spr_vel, date='2022-02-02', training_drill='CROSSING-AND-FINISHING-HSR-SPR')\n",
    "df_training_attack_vs_defence_attack_superiority_physical_report = create_physical_report_per_training_session(df_training_attack_vs_defence_attack_superiority_vel, date='2022-02-02', training_drill='ATTACK-VS-DEFENCE-ATTACK-SUPERIORITY')\n",
    "df_training_full_session_modified_physical_report = create_physical_report_per_training_session(df_training_full_session_modified_vel, date='2022-02-02', training_drill='FULL-SESSION-MODIFIED')\n",
    "df_training_passing_drill_physical_physical_report = create_physical_report_per_training_session(df_training_passing_drill_physical_vel, date='2022-02-02', training_drill='PASSING-DRILL-PHYSICAL')\n",
    "df_training_warm_up_coordination_agility_physical_report = create_physical_report_per_training_session(df_training_warm_up_coordination_agility_vel, date='2022-02-02', training_drill='WARM-UP-COORDINATION-AGILITY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4.6'></a>\n",
    "\n",
    "### <a id='#section4.6'>4.6. Create Single Physical Report for the Day of Interest</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate a bespoke physical summary of all the players for an individual training session\n",
    "def create_physical_report_per_day(date):\n",
    "    \n",
    "    \"\"\"\n",
    "    Define a function to generate a bespoke physical summary of all the players for an individual training session\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Read in exported CSV file if exists, if not, download the latest JSON data\n",
    "    if not os.path.exists(os.path.join(data_dir_physical, 'engineered', 'Set 2', f'{date}-ALL-TRAINING-SESSIONS-PHYSICAL-REPORT-ALL-PLAYERS.csv')):\n",
    "    \n",
    "    \n",
    "        ### Start timer\n",
    "        tic = datetime.datetime.now()\n",
    "\n",
    "        \n",
    "        ### Print time of engineering of tracking data started\n",
    "        print(f'Creation a single training report for {date} started at: {tic}')        \n",
    "\n",
    "\n",
    "        ### List all files available\n",
    "        lst_all_files = glob.glob(os.path.join(data_dir_physical, 'engineered', 'Set 2', f'{date}*-PHYSICAL-REPORT-ALL-PLAYERS.csv'))\n",
    "\n",
    "\n",
    "        ### Create an empty list to append individual DataFrames\n",
    "        lst_files_to_append =[]\n",
    "\n",
    "\n",
    "        ### Iterate through each file in list of all files\n",
    "        for file in lst_all_files:\n",
    "\n",
    "            ### Create temporary DataFrame with each file\n",
    "            df_temp = pd.read_csv(file, index_col=None, header=0)\n",
    "\n",
    "            ### Append each individual Define each individual file to the empty list (to be concatenated) \n",
    "            lst_files_to_append.append(df_temp)\n",
    "\n",
    "\n",
    "        ### Concatenate all the files\n",
    "        df_day_training_report = pd.concat(lst_files_to_append, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "        ### Save DataFrame\n",
    "\n",
    "        #### Define filename for each combined file to be saved\n",
    "        save_filename = f'{date}-ALL-TRAINING-SESSIONS-PHYSICAL-REPORT-ALL-PLAYERS'.replace(' ', '-').replace('(', '').replace(')', '').replace(':', '').replace('.', '').replace('__', '_').upper()\n",
    "\n",
    "        #### Define the filepath to save each combined file\n",
    "        path = os.path.join(data_dir_physical, 'engineered', 'Set 2')\n",
    "\n",
    "        #### Save the combined file as a CSV\n",
    "        df_day_training_report.to_csv(path + f'/{save_filename}.csv', index=None, header=True)\n",
    "\n",
    "\n",
    "        ### Engineer the data\n",
    "\n",
    "        #### \n",
    "        df_day_training_report['Date'] = date\n",
    "    \n",
    "    \n",
    "        ### End timer\n",
    "        toc = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "        ### Print time reading of CSV files end\n",
    "        print(f'Creation a single training report for {date} ended at: {toc}')\n",
    "\n",
    "\n",
    "        ### Calculate time take\n",
    "        total_time = (toc-tic).total_seconds()\n",
    "        print(f'Time taken create a single training report for {date} is: {total_time:0.2f} seconds.')\n",
    "\n",
    "    \n",
    "    \n",
    "    ## If CSV file already exists, read in previously saved DataFrame\n",
    "    else:\n",
    "        \n",
    "        ### Print time reading of CSV files started\n",
    "        print('CSV file already saved to local storage. Reading in file as a pandas DataFrame.')\n",
    "        \n",
    "        ### Read in raw DataFrame\n",
    "        df_day_training_report = pd.read_csv(os.path.join(data_dir_physical, 'engineered', 'Set 2', f'{date}-ALL-TRAINING-SESSIONS-PHYSICAL-REPORT-ALL-PLAYERS.csv'))\n",
    "\n",
    "    \n",
    "    ## Return DataFrame\n",
    "    return df_day_training_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_report_02022022 = create_physical_report_per_day(date='2022-02-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_report_02022022.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_report_02022022.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section5'></a>\n",
    "\n",
    "## <a id='#section5'>5. Summary</a>\n",
    "This notebook engineer physical data using [pandas](http://pandas.pydata.org/) to create a series of training reports for players, determining metrics include distance covered, total sprints, top speeds, amoungst other breakdowns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section6'></a>\n",
    "\n",
    "## <a id='#section6'>6. Next Steps</a>\n",
    "The next stage is to visualise this data in Tableau and analyse the findings, to be presented in a deck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section7'></a>\n",
    "\n",
    "## <a id='#section7'>7. References</a>\n",
    "*    [Application of Individualized Speed Zones to Quantify External Training Load in Professional Soccer](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7126260/) by Vincenzo Rago, João Brito, Pedro Figueiredo, Peter Krustrup, and António Rebelo.\n",
    "*    [Laurie Shaw](https://twitter.com/EightyFivePoint)'s Metrica Sports Tracking data libraries, [`LaurieOnTracking`](https://github.com/Friends-of-Tracking-Data-FoTD/LaurieOnTracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***Visit my website [eddwebster.com](https://www.eddwebster.com) or my [GitHub Repository](https://github.com/eddwebster) for more projects. If you'd like to get in contact, my Twitter handle is [@eddwebster](http://www.twitter.com/eddwebster) and my email is: edd.j.webster@gmail.com.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 642,
   "position": {
    "height": "40px",
    "left": "1118px",
    "right": "20px",
    "top": "-7px",
    "width": "489px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
